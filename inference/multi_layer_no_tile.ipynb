{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.utils import ContextManagers\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel, DDIMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import check_min_version, deprecate, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"tokenizer\"\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"text_encoder\"\n",
    ").cuda()\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"models/12to14\", subfolder=\"vae\"\n",
    ").cuda()\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"models/16to18\", subfolder=\"unet\"\n",
    ").cuda()\n",
    "scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"scheduler\"\n",
    ")\n",
    "low_res_scheduler = DDPMScheduler.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"low_res_scheduler\"\n",
    ")\n",
    "\n",
    "def get_tokenized_caption(caption):\n",
    "    captions = [caption]\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(lrim, unet, timesteps=30, negative=False, guidance_scale_n=0, positive=False, guidance_scale_p=0):\n",
    "    with torch.no_grad():\n",
    "        weight_dtype = torch.float32\n",
    "        latents = torch.randn((1, 4, 128, 128)).to('cuda')\n",
    "\n",
    "        prompt_embeds = text_encoder(torch.unsqueeze(get_tokenized_caption(\"satellite photo\")[0], dim=0).to('cuda'))[0]\n",
    "        if negative:\n",
    "            neg_embeds = text_encoder(torch.unsqueeze(get_tokenized_caption(\"blurry, lowres, low quality, deformed\")[0], dim=0).to('cuda'))[0]\n",
    "            prompt_embeds = torch.cat([prompt_embeds, neg_embeds])\n",
    "        \n",
    "        if positive:\n",
    "            pos_embeds = text_encoder(torch.unsqueeze(get_tokenized_caption(\"sharp, high quality, detailed, realistic\")[0], dim=0).to('cuda'))[0]\n",
    "            prompt_embeds = torch.cat([prompt_embeds, pos_embeds])\n",
    "\n",
    "        scheduler.set_timesteps(timesteps, device='cuda')\n",
    "        timesteps = scheduler.timesteps\n",
    "\n",
    "        for i, t in enumerate(timesteps):\n",
    "            latent_inp = torch.cat([latents]*2) if negative else latents\n",
    "            latent_inp = scheduler.scale_model_input(latent_inp, t)\n",
    "            latent_inp = torch.cat([latent_inp, torch.unsqueeze(lrim, dim=0).repeat((2 if negative else 1, 1, 1, 1))], dim=1)\n",
    "            noise_pred = unet(latent_inp, t, prompt_embeds, class_labels=noise_level, return_dict=False)[0]\n",
    "\n",
    "            if negative and not positive:\n",
    "                noise_pred_text, noise_pred_neg = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_text + guidance_scale_n * (noise_pred_text - noise_pred_neg)\n",
    "            \n",
    "            if positive and not negative:\n",
    "                noise_pred_text, noise_pred_pos = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_text + guidance_scale_p * (noise_pred_pos - noise_pred_text)\n",
    "            \n",
    "            if positive and negative:\n",
    "                noise_pred_text, noise_pred_neg = noise_pred.chunk(3)\n",
    "                noise_pred = noise_pred_text + guidance_scale_n * (noise_pred_text - noise_pred_neg) + guidance_scale_p * (noise_pred_pos - noise_pred_text)\n",
    "\n",
    "            latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "        result = vae.decode(latents/vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "    return result[0]#, mins, maxs, nns, ops, means, stds\n",
    "\n",
    "def get_lrim(row, col, low_res):\n",
    "    lr_center_crop = low_res.crop(((low_res.width*3)//8, (low_res.width*3)//8, (low_res.width*5)//8, (low_res.width*5)//8))\n",
    "    lrim =  lr_center_crop.crop((tile_dim//8*col, tile_dim//8*row, tile_dim//8*(col+2), tile_dim//8*(row+2)))\n",
    "    return train_transforms(lrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel, StableDiffusionUpscalePipeline\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_dirs = os.listdir('/u/ansh/cascaded-generative/bing_20/')\n",
    "\n",
    "hr_zoom = 20\n",
    "\n",
    "transform = torchvision.transforms.ToPILImage()\n",
    "noise_level = torch.tensor([2], dtype=torch.long, device='cuda')\n",
    "\n",
    "def generate_sample(lrim, timesteps, uuid, unet, negative=False, guidance_scale_n=0, positive=False, guidance_scale_p=0):\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir + f'{hr_zoom}/{lrw}_{thr}'):\n",
    "            os.makedirs(save_dir + f'{hr_zoom}/{lrw}_{thr}')    \n",
    "        \n",
    "    image = get_image(lrim, unet, timesteps = timesteps, negative=negative, guidance_scale_n=guidance_scale_n, positive=positive, guidance_scale_p=guidance_scale_p)\n",
    "    img = transform(torch.clip((image+1)/2, 0, 1))\n",
    "    if show:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    if save:\n",
    "        img.save(save_dir + f'{hr_zoom}/{timesteps}/{i}.png')\n",
    "    return img\n",
    "\n",
    "def get_random_image():\n",
    "    root_dir = '/scratch/bbut/bing_datasets/'\n",
    "    dataset = random.choice(['bing_20', 'bing_urban'])\n",
    "    uuid = random.choice(os.listdir(root_dir + dataset))\n",
    "    path_to_image = root_dir + dataset + '/' + uuid + '/'\n",
    "    return path_to_image, uuid\n",
    "\n",
    "def load_random_lr():\n",
    "    image_path, uuid = get_random_image()\n",
    "    lr_image = Image.open(image_path+f'/{hr_zoom-6}.jpg').convert(\"RGB\")\n",
    "    lr_image = lr_image.crop(((lr_image.width*3)//8, (lr_image.width*3)//8, (lr_image.width*5)//8, (lr_image.width*5)//8))\n",
    "    lr_image = lr_image.crop((0, 0, 512//4, 512//4))\n",
    "    lrim = train_transforms(lr_image).cuda()\n",
    "    return lrim\n",
    "\n",
    "unet10 = UNet2DConditionModel.from_pretrained(\n",
    "    \"models/10to12\", subfolder=\"unet\"\n",
    ").cuda()\n",
    "unet12 = UNet2DConditionModel.from_pretrained(\n",
    "    \"models/12to14\", subfolder=\"unet\"\n",
    ").cuda()\n",
    "unet14 = UNet2DConditionModel.from_pretrained(\n",
    "    \"models/14to16\", subfolder=\"unet\"\n",
    ").cuda()\n",
    "unet16 = UNet2DConditionModel.from_pretrained(\n",
    "    \"models/16to18\", subfolder=\"unet\"\n",
    ").cuda()\n",
    "unet18 = UNet2DConditionModel.from_pretrained(\n",
    "    \"models/18to20\", subfolder=\"unet\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_be_used = []\n",
    "for i in range(20):\n",
    "    images_to_be_used.append(get_random_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler, DEISMultistepScheduler, DDIMScheduler, DPMSolverSDEScheduler, PNDMScheduler, UniPCMultistepScheduler\n",
    "save_dir = '/scratch/bbut/ansh/output_comparisons/'\n",
    "save = False\n",
    "show = True\n",
    "\n",
    "config = {'num_train_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02, \n",
    "        'beta_schedule': 'scaled_linear', 'trained_betas': None, 'clip_sample': False, \n",
    "        'set_alpha_to_one': False, 'steps_offset': 1, 'prediction_type': 'v_prediction', 'thresholding': False, \n",
    "        'dynamic_thresholding_ratio': 0.995, 'clip_sample_range': 1.0, \n",
    "        'sample_max_value': 1.0, 'timestep_spacing': 'leading', \n",
    "        'rescale_betas_zero_snr': False, \n",
    "        '_use_default_values': ['clip_sample_range', 'rescale_betas_zero_snr', 'thresholding', 'sample_max_value', 'timestep_spacing', 'dynamic_thresholding_ratio']}\n",
    "\n",
    "scheduler = DPMSolverSinglestepScheduler().from_config(config)\n",
    "timesteps = 50\n",
    "images = []\n",
    "images_2 = []\n",
    "images_5 = []\n",
    "lr_images = []\n",
    "hr_images = []\n",
    "start = time.time()\n",
    "for i in range(len(images_to_be_used)):\n",
    "    # if(i%10== 0):\n",
    "    #     print(i)\n",
    "\n",
    "    # Low res image\n",
    "    image_path, uuid = images_to_be_used[i]\n",
    "    lr_image = Image.open(image_path+f'/{hr_zoom-10}.jpg').convert(\"RGB\")\n",
    "    lr_image = lr_image.crop(((lr_image.width*3)//8, (lr_image.width*3)//8, (lr_image.width*5)//8, (lr_image.width*5)//8))\n",
    "    lr_image = lr_image.crop((192, 192, 320, 320))\n",
    "    lrim = train_transforms(lr_image).cuda()\n",
    "\n",
    "    print(\"No negative prompt\")\n",
    "    # Super res 3 times\n",
    "    img12 = train_transforms(generate_sample(lrim, timesteps, uuid, unet10).crop((192, 192, 320, 320))).cuda()\n",
    "    img14 = train_transforms(generate_sample(img12, timesteps, uuid, unet12).crop((192, 192, 320, 320))).cuda()\n",
    "    img16 = train_transforms(generate_sample(img14, timesteps, uuid, unet14).crop((192, 192, 320, 320))).cuda()\n",
    "    img18 = train_transforms(generate_sample(img16, timesteps, uuid, unet16).crop((192, 192, 320, 320))).cuda()\n",
    "    img20 = generate_sample(img18, timesteps, uuid, unet18)\n",
    "\n",
    "    print(\"Negative Conditioning - 52334\")\n",
    "    # Super res 3 times\n",
    "    img12 = train_transforms(generate_sample(lrim, timesteps, uuid, unet10, True, 5).crop((192, 192, 320, 320))).cuda()\n",
    "    img14 = train_transforms(generate_sample(img12, timesteps, uuid, unet12, True, 2).crop((192, 192, 320, 320))).cuda()\n",
    "    img16 = train_transforms(generate_sample(img14, timesteps, uuid, unet14, True, 3).crop((192, 192, 320, 320))).cuda()\n",
    "    img18 = train_transforms(generate_sample(img16, timesteps, uuid, unet16, True, 3).crop((192, 192, 320, 320))).cuda()\n",
    "    img20 = generate_sample(img18, timesteps, uuid, unet18, True, 4)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(lr_image)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Corresponding HR Ground Truth \n",
    "    gt = Image.open(image_path+f'/{hr_zoom}.jpg').convert(\"RGB\")\n",
    "    gt = gt.crop((768, 768, 1280, 1280))\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir + f'{hr_zoom}/gt'):\n",
    "            os.makedirs(save_dir + f'{hr_zoom}/gt')\n",
    "        gt.save(save_dir+f'{hr_zoom}/gt/{i}.png')\n",
    "    if show:\n",
    "        plt.imshow(gt)\n",
    "        plt.show()\n",
    "\n",
    "    images.append(img20)\n",
    "    lr_images.append(lr_image)\n",
    "    hr_images.append(gt)\n",
    "\n",
    "    if(i%100 == 0):\n",
    "        print((time.time() - start)/(i+1))\n",
    "print(f\"timesteps {timesteps}\")\n",
    "np_images = np.stack([np.asarray(img) for img in images])\n",
    "grid_img = torchvision.utils.make_grid(torch.Tensor(np_images).permute(0, 3, 1, 2), nrow=5)\n",
    "plt.imshow(grid_img.permute(1, 2, 0)/255)\n",
    "plt.show()\n",
    "print(\"high res\")\n",
    "np_images = np.stack([np.asarray(img) for img in hr_images])\n",
    "grid_img = torchvision.utils.make_grid(torch.Tensor(np_images).permute(0, 3, 1, 2), nrow=5)\n",
    "plt.imshow(grid_img.permute(1, 2, 0)/255)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthgen10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
