{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5491b-747d-43a0-b60d-17a5dfc991d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.utils import ContextManagers\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel, DDIMScheduler, ControlNetModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import check_min_version, deprecate, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6744f-0867-404e-8190-b32c3bad4213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_folder = '/scratch/bbut/prathi3/unconditional/10_50000'\n",
    "root_folder = 'models/10gen'\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    root_folder, subfolder=\"tokenizer\"\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    root_folder, subfolder=\"text_encoder\"\n",
    ").cuda()\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    root_folder, subfolder=\"vae\"\n",
    ").cuda()\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    root_folder, subfolder=\"unet\"\n",
    ").cuda()\n",
    "scheduler = DDIMScheduler.from_pretrained(\n",
    "    root_folder, subfolder=\"scheduler\"\n",
    ")\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"models/10gen\", subfolder=\"controlnet\"\n",
    ").cuda()\n",
    "\n",
    "# controlnet = ControlNetModel.from_unet(unet).cuda()\n",
    "\n",
    "def get_tokenized_caption(caption, tokenizer):\n",
    "    captions = [caption]\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "from diffusers import StableDiffusionUpscalePipeline, DPMSolverSinglestepScheduler\n",
    "\n",
    "pipeline = StableDiffusionUpscalePipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\")\n",
    "scheduler = DPMSolverSinglestepScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fd6e6-be5c-407f-a454-f7543ca94235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()\n",
    "def get_controlnet_image(cond=None, use_lr=False, use_ctrlnet=True, use_neg=True, use_pos=False):\n",
    "    transform = torchvision.transforms.ToPILImage()\n",
    "    noise_level = torch.tensor([20], dtype=torch.long, device='cuda')\n",
    "    weight_dtype = torch.float32\n",
    "    latents = torch.randn((1, 4, 64, 64)).to('cuda')\n",
    "    \n",
    "    repeats = 1\n",
    "    if use_neg:\n",
    "        repeats +=1\n",
    "    if use_pos:\n",
    "        repeats +=1\n",
    "    \n",
    "    if use_lr:\n",
    "        latents = torch.randn((1, 4, 128, 128)).to('cuda')\n",
    "        lrim = -torch.ones((1, 3, 128, 128)).cuda()\n",
    "\n",
    "    lrim = lrim.repeat((repeats, 1, 1, 1))\n",
    "    cond = cond.repeat((repeats, 1, 1, 1))\n",
    "    \n",
    "    prompt_embeds = text_encoder(torch.unsqueeze(get_tokenized_caption(\"satellite photo\", tokenizer)[0], dim=0).to('cuda'))[0]\n",
    "    if use_neg:\n",
    "        neg_embeds = text_encoder(torch.unsqueeze(get_tokenized_caption(\"blurry, lowres, low quality\", tokenizer)[0], dim=0).to('cuda'))[0]\n",
    "        prompt_embeds = torch.cat([prompt_embeds, neg_embeds])\n",
    "    if use_pos:\n",
    "        pos_embeds = text_encoder(torch.unsqueeze(get_tokenized_caption(\"fall colors\", tokenizer)[0], dim=0).to('cuda'))[0]\n",
    "        prompt_embeds = torch.cat([prompt_embeds, pos_embeds])        \n",
    "\n",
    "    scheduler.set_timesteps(50, device='cuda')\n",
    "    timesteps = scheduler.timesteps\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, t in enumerate(timesteps):\n",
    "            \n",
    "            latent_inp = torch.cat([latents]*repeats)\n",
    "            latent_inp = scheduler.scale_model_input(latent_inp, t)\n",
    "\n",
    "            if use_lr:\n",
    "                latent_inp = torch.cat([latent_inp, lrim], dim=1)\n",
    "                # cond = cond.resize((cond_image.width*2, cond_image.height*2))\n",
    "                \n",
    "            if use_ctrlnet:\n",
    "                down_block_res, mid_block_res = controlnet(latent_inp, t, prompt_embeds, class_labels=noise_level, controlnet_cond=cond, return_dict=False)\n",
    "                \n",
    "                noise_pred = unet(latent_inp, t, prompt_embeds, class_labels=noise_level, down_block_additional_residuals=[\n",
    "                            sample.to(dtype=weight_dtype) for sample in down_block_res\n",
    "                        ], mid_block_additional_residual=mid_block_res.to(dtype=weight_dtype)).sample\n",
    "            else:\n",
    "                noise_pred = unet(latent_inp, t, prompt_embeds, class_labels=noise_level, return_dict=False)[0]\n",
    "\n",
    "            if use_neg and not use_pos:\n",
    "                noise_pred_text, noise_pred_neg = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_text + 3.5 * (noise_pred_text - noise_pred_neg)\n",
    "            if use_neg and use_pos:\n",
    "                noise_pred_text, noise_pred_neg, noise_pred_pos = noise_pred.chunk(3)\n",
    "                noise_pred = noise_pred_text + 3 * (noise_pred_text - noise_pred_neg) + 5 * (noise_pred_pos - noise_pred_neg)\n",
    "                \n",
    "            latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "        \n",
    "        result = vae.decode(latents/vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "        return transform(torch.clamp(result[0]*0.5+0.5, 0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa3f2a-79e9-47de-93e4-561d73fc85a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "searchdir = '/scratch/bbut/bing_datasets/bing_20/'\n",
    "\n",
    "for k in range(100):\n",
    "    example_imdir = random.choice(os.listdir(searchdir))\n",
    "    cond_og = Image.open(f\"{searchdir}/{example_imdir}/10_map.jpg\")\n",
    "    cond_og = cond_og.crop(((cond_og.width * 3) // 8, (cond_og.width * 3) // 8, (cond_og.width * 5) // 8, (cond_og.width * 5) // 8))\n",
    "    cond_image = cond_og.resize((cond_og.width * 2, cond_og.height * 2))\n",
    "    img = Image.open(f\"{searchdir}/{example_imdir}/10.jpg\")\n",
    "    img = img.crop(((img.width * 3) // 8, (img.width * 3) // 8, (img.width * 5) // 8, (img.width * 5) // 8))\n",
    "\n",
    "    cond = train_transforms(cond_image).cuda()\n",
    "    images = []\n",
    "\n",
    "    images.append(cond_og)\n",
    "    \n",
    "    # Generate and store the images from get_controlnet_image\n",
    "    for i in range(1):\n",
    "        res = get_controlnet_image(cond, use_lr=True, use_neg=False)\n",
    "        images.append(res)\n",
    "    \n",
    "\n",
    "    # Calculate the total width of the combined image\n",
    "    total_width = sum(img.width for img in images)\n",
    "\n",
    "    # Create a new image with the combined width and the maximum height\n",
    "    combined_image = Image.new('RGB', (total_width, max(img.height for img in images)))\n",
    "\n",
    "    # Paste the individual images onto the combined image\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        combined_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "\n",
    "    # Save the combined image\n",
    "    save_path = os.path.join(f\"consistency/{k}.jpg\")\n",
    "    combined_image.save(save_path)\n",
    "\n",
    "    # print(searchdir, example_imdir)\n",
    "    # plt.imshow(np.array(combined_image))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc93803-51d1-4bf5-8bda-82de409c5504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "searchdir = '/scratch/bbut/bing_datasets/bing_20/'\n",
    "for i in range(50):\n",
    "    example_imdir = random.choice(os.listdir(searchdir))\n",
    "    cond_og = Image.open(f\"{searchdir}/{example_imdir}/10_map.jpg\")\n",
    "    cond_og = cond_og.crop(((cond_og.width * 3) // 8, (cond_og.width * 3) // 8, (cond_og.width * 5) // 8, (cond_og.width * 5) // 8))\n",
    "    cond_image = cond_og.resize((cond_og.width * 2, cond_og.height * 2))\n",
    "\n",
    "    actual = Image.open(f\"{searchdir}/{example_imdir}/10.jpg\")\n",
    "    actual = actual.crop(((actual.width * 3) // 8, (actual.width * 3) // 8, (actual.width * 5) // 8, (actual.width * 5) // 8))\n",
    "\n",
    "    example_imdir = random.choice(os.listdir(searchdir))\n",
    "    randomim = Image.open(f\"{searchdir}/{example_imdir}/10.jpg\")\n",
    "    randomim = randomim.crop(((randomim.width * 3) // 8, (randomim.width * 3) // 8, (randomim.width * 5) // 8, (randomim.width * 5) // 8))\n",
    "\n",
    "    images = []\n",
    "\n",
    "    images.append(cond_og)\n",
    "    images.append(actual)\n",
    "    # images.append(randomim)\n",
    "\n",
    "    # Calculate the total width of the combined image\n",
    "    total_width = sum(img.width for img in images)\n",
    "\n",
    "    # Create a new image with the combined width and the maximum height\n",
    "    combined_image = Image.new('RGB', (total_width, max(img.height for img in images)))\n",
    "\n",
    "    # Paste the individual images onto the combined image\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        combined_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "\n",
    "    # Save the combined image\n",
    "    save_path = os.path.join(f\"consistency_gts/{i}.jpg\")\n",
    "    combined_image.save(save_path)\n",
    "    print(searchdir, example_imdir)\n",
    "    # plt.imshow(np.array(combined_image))\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthgen10.0",
   "language": "python",
   "name": "earthgen10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
